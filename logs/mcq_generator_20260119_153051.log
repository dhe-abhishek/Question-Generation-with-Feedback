2026-01-19 15:30:52,079 - matplotlib - DEBUG - matplotlib data path: D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\matplotlib\mpl-data
2026-01-19 15:30:52,090 - matplotlib - DEBUG - CONFIGDIR=C:\Users\Administrator\.matplotlib
2026-01-19 15:30:52,090 - matplotlib - DEBUG - interactive is False
2026-01-19 15:30:52,091 - matplotlib - DEBUG - platform is win32
2026-01-19 15:30:52,147 - matplotlib - DEBUG - CACHEDIR=C:\Users\Administrator\.matplotlib
2026-01-19 15:30:52,149 - matplotlib.font_manager - DEBUG - Using fontManager instance from C:\Users\Administrator\.matplotlib\fontlist-v390.json
2026-01-19 15:30:55,052 - datasets - DEBUG - PyTorch version 2.9.0 available.
2026-01-19 15:30:58,679 - ragas._analytics - DEBUG - Starting AnalyticsBatcher thread with interval 10 seconds
2026-01-19 15:30:59,825 - relevancy_utils - INFO - Starting Ragas scorer initialization...
2026-01-19 15:31:00,463 - relevancy_utils - DEBUG - ChatGroq LLM initialized.
2026-01-19 15:31:02,026 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu
2026-01-19 15:31:02,026 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2026-01-19 15:31:02,028 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2026-01-19 15:31:02,574 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-19 15:31:02,595 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-19 15:31:02,813 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-19 15:31:02,832 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-19 15:31:03,057 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 307 0
2026-01-19 15:31:03,078 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config_sentence_transformers.json HTTP/1.1" 200 0
2026-01-19 15:31:03,312 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 307 0
2026-01-19 15:31:03,331 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/README.md HTTP/1.1" 200 0
2026-01-19 15:31:03,548 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 307 0
2026-01-19 15:31:03,565 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/modules.json HTTP/1.1" 200 0
2026-01-19 15:31:03,792 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 307 0
2026-01-19 15:31:03,810 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/sentence_bert_config.json HTTP/1.1" 200 0
2026-01-19 15:31:04,040 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
2026-01-19 15:31:04,254 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 307 0
2026-01-19 15:31:04,276 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/config.json HTTP/1.1" 200 0
2026-01-19 15:31:04,603 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 307 0
2026-01-19 15:31:04,622 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/tokenizer_config.json HTTP/1.1" 200 0
2026-01-19 15:31:04,848 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2/tree/main/additional_chat_templates?recursive=False&expand=False HTTP/1.1" 404 64
2026-01-19 15:31:05,120 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/1_Pooling/config.json HTTP/1.1" 307 0
2026-01-19 15:31:05,139 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "HEAD /api/resolve-cache/models/sentence-transformers/all-MiniLM-L6-v2/c9745ed1d9f207416be6d2e6f8de32d1f16199bf/1_Pooling%2Fconfig.json HTTP/1.1" 200 0
2026-01-19 15:31:05,369 - urllib3.connectionpool - DEBUG - https://huggingface.co:443 "GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1" 200 6876
2026-01-19 15:31:05,374 - relevancy_utils - DEBUG - HuggingFace Embeddings initialized.
2026-01-19 15:31:05,374 - relevancy_utils - INFO - Ragas ResponseRelevancy scorer initialized successfully.
2026-01-19 15:31:05,376 - faithfulness_utils - INFO - Initializing Faithfulness scorer...
2026-01-19 15:31:06,011 - faithfulness_utils - INFO - Faithfulness scorer initialized successfully.
2026-01-19 15:31:06,025 - werkzeug - WARNING -  * Debugger is active!
2026-01-19 15:31:06,028 - werkzeug - INFO -  * Debugger PIN: 103-783-783
2026-01-19 15:31:51,004 - routes - INFO - Rendering home page
2026-01-19 15:31:51,017 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:31:51] "GET / HTTP/1.1" 200 -
2026-01-19 15:31:53,068 - routes - INFO - Rendering Question generator page
2026-01-19 15:31:53,075 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:31:53] "GET /mcq-generator HTTP/1.1" 200 -
2026-01-19 15:32:01,305 - routes - INFO - Received Question generation request
2026-01-19 15:32:01,316 - validation - INFO - File and parameters validated successfully.
2026-01-19 15:32:01,317 - file_utils - DEBUG - File 03-storage1.pdf allowed: True
2026-01-19 15:32:01,323 - file_utils - INFO - File saved to: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:32:01,433 - pdf_extraction_util - INFO - Successfully extracted 26430 characters from 03-storage1.pdf
2026-01-19 15:32:02,153 - question_generation - INFO - QuestionGenerator initialized with model: gemini-2.5-flash
2026-01-19 15:32:02,154 - question_generation - INFO - Generating structured content using model gemini-2.5-flash...
2026-01-19 15:32:02,154 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2026-01-19 15:32:02,156 - httpcore.connection - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None
2026-01-19 15:32:02,167 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC6285A90>
2026-01-19 15:32:02,167 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023EC6071A90> server_hostname='generativelanguage.googleapis.com' timeout=None
2026-01-19 15:32:02,182 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC60BF610>
2026-01-19 15:32:02,182 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2026-01-19 15:32:02,182 - httpcore.http11 - DEBUG - send_request_headers.complete
2026-01-19 15:32:02,182 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2026-01-19 15:32:02,182 - httpcore.http11 - DEBUG - send_request_body.complete
2026-01-19 15:32:02,182 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2026-01-19 15:32:04,000 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 503, b'Service Unavailable', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 19 Jan 2026 10:02:03 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=1806'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-19 15:32:04,000 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 503 Service Unavailable"
2026-01-19 15:32:04,000 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2026-01-19 15:32:04,000 - httpcore.http11 - DEBUG - receive_response_body.complete
2026-01-19 15:32:04,000 - httpcore.http11 - DEBUG - response_closed.started
2026-01-19 15:32:04,001 - httpcore.http11 - DEBUG - response_closed.complete
2026-01-19 15:32:04,001 - question_generation - ERROR - Gemini API Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
Traceback (most recent call last):
  File "D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\app\services\question_generation.py", line 270, in _generate_structured_content
    response = self.client.models.generate_content(
        model=self.model_name,
        contents=[prompt],
        config=generation_config
    )
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\models.py", line 4983, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\models.py", line 3795, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\_api_client.py", line 1292, in request
    response = self._request(http_request, http_options, stream=False)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\_api_client.py", line 1128, in _request
    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\_api_client.py", line 1105, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\errors.py", line 110, in raise_for_response
    raise ServerError(status_code, response_json, response)
google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
2026-01-19 15:32:04,008 - file_utils - INFO - Cleaned up file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:32:04,008 - routes - ERROR - Question generation failed: Gemini API Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
2026-01-19 15:32:04,009 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:32:04] "[32mPOST /generate HTTP/1.1[0m" 302 -
2026-01-19 15:32:04,014 - routes - INFO - Rendering Question generator page
2026-01-19 15:32:04,015 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:32:04] "GET /mcq-generator HTTP/1.1" 200 -
2026-01-19 15:33:24,087 - routes - INFO - Received Question generation request
2026-01-19 15:33:24,098 - validation - INFO - File and parameters validated successfully.
2026-01-19 15:33:24,098 - file_utils - DEBUG - File 04-bufferpool.pdf allowed: True
2026-01-19 15:33:24,106 - file_utils - INFO - File saved to: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\04-bufferpool.pdf
2026-01-19 15:33:24,242 - pdf_extraction_util - INFO - Successfully extracted 42260 characters from 04-bufferpool.pdf
2026-01-19 15:33:25,017 - question_generation - INFO - QuestionGenerator initialized with model: gemini-2.5-flash
2026-01-19 15:33:25,017 - question_generation - INFO - Generating structured content using model gemini-2.5-flash...
2026-01-19 15:33:25,017 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2026-01-19 15:33:25,019 - httpcore.connection - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None
2026-01-19 15:33:25,046 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC61C6C10>
2026-01-19 15:33:25,046 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023EC60736B0> server_hostname='generativelanguage.googleapis.com' timeout=None
2026-01-19 15:33:25,061 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC61CDBA0>
2026-01-19 15:33:25,061 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2026-01-19 15:33:25,061 - httpcore.http11 - DEBUG - send_request_headers.complete
2026-01-19 15:33:25,061 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2026-01-19 15:33:25,062 - httpcore.http11 - DEBUG - send_request_body.complete
2026-01-19 15:33:25,062 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2026-01-19 15:33:39,583 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 503, b'Service Unavailable', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 19 Jan 2026 10:03:39 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=14510'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-19 15:33:39,584 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 503 Service Unavailable"
2026-01-19 15:33:39,584 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2026-01-19 15:33:39,585 - httpcore.http11 - DEBUG - receive_response_body.complete
2026-01-19 15:33:39,585 - httpcore.http11 - DEBUG - response_closed.started
2026-01-19 15:33:39,586 - httpcore.http11 - DEBUG - response_closed.complete
2026-01-19 15:33:39,586 - question_generation - ERROR - Gemini API Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
Traceback (most recent call last):
  File "D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\app\services\question_generation.py", line 270, in _generate_structured_content
    response = self.client.models.generate_content(
        model=self.model_name,
        contents=[prompt],
        config=generation_config
    )
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\models.py", line 4983, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\models.py", line 3795, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\_api_client.py", line 1292, in request
    response = self._request(http_request, http_options, stream=False)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\_api_client.py", line 1128, in _request
    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\_api_client.py", line 1105, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\errors.py", line 110, in raise_for_response
    raise ServerError(status_code, response_json, response)
google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
2026-01-19 15:33:39,590 - file_utils - INFO - Cleaned up file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\04-bufferpool.pdf
2026-01-19 15:33:39,590 - routes - ERROR - Question generation failed: Gemini API Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
2026-01-19 15:33:39,593 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:33:39] "[32mPOST /generate HTTP/1.1[0m" 302 -
2026-01-19 15:33:39,600 - routes - INFO - Rendering Question generator page
2026-01-19 15:33:39,601 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:33:39] "GET /mcq-generator HTTP/1.1" 200 -
2026-01-19 15:33:51,710 - routes - INFO - Received Question generation request
2026-01-19 15:33:51,722 - validation - INFO - File and parameters validated successfully.
2026-01-19 15:33:51,723 - file_utils - DEBUG - File 03-storage1.pdf allowed: True
2026-01-19 15:33:51,730 - file_utils - INFO - File saved to: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:33:51,830 - pdf_extraction_util - INFO - Successfully extracted 26430 characters from 03-storage1.pdf
2026-01-19 15:33:52,771 - question_generation - INFO - QuestionGenerator initialized with model: gemini-2.5-flash
2026-01-19 15:33:52,774 - question_generation - INFO - Generating structured content using model gemini-2.5-flash...
2026-01-19 15:33:52,776 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2026-01-19 15:33:52,778 - httpcore.connection - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None
2026-01-19 15:33:52,793 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC61CF360>
2026-01-19 15:33:52,795 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023EC62A8050> server_hostname='generativelanguage.googleapis.com' timeout=None
2026-01-19 15:33:52,811 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC6290EF0>
2026-01-19 15:33:52,811 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2026-01-19 15:33:52,811 - httpcore.http11 - DEBUG - send_request_headers.complete
2026-01-19 15:33:52,811 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2026-01-19 15:33:52,811 - httpcore.http11 - DEBUG - send_request_body.complete
2026-01-19 15:33:52,811 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2026-01-19 15:34:09,835 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 503, b'Service Unavailable', [(b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Type', b'application/json; charset=UTF-8'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 19 Jan 2026 10:04:09 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=17011'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-19 15:34:09,835 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 503 Service Unavailable"
2026-01-19 15:34:09,836 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2026-01-19 15:34:09,836 - httpcore.http11 - DEBUG - receive_response_body.complete
2026-01-19 15:34:09,837 - httpcore.http11 - DEBUG - response_closed.started
2026-01-19 15:34:09,837 - httpcore.http11 - DEBUG - response_closed.complete
2026-01-19 15:34:09,837 - question_generation - ERROR - Gemini API Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
Traceback (most recent call last):
  File "D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\app\services\question_generation.py", line 270, in _generate_structured_content
    response = self.client.models.generate_content(
        model=self.model_name,
        contents=[prompt],
        config=generation_config
    )
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\models.py", line 4983, in generate_content
    response = self._generate_content(
        model=model, contents=contents, config=parsed_config
    )
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\models.py", line 3795, in _generate_content
    response = self._api_client.request(
        'post', path, request_dict, http_options
    )
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\_api_client.py", line 1292, in request
    response = self._request(http_request, http_options, stream=False)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\_api_client.py", line 1128, in _request
    return self._retry(self._request_once, http_request, stream)  # type: ignore[no-any-return]
           ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 477, in __call__
    do = self.iter(retry_state=retry_state)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 378, in iter
    result = action(retry_state)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 420, in exc_check
    raise retry_exc.reraise()
          ~~~~~~~~~~~~~~~~~^^
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 187, in reraise
    raise self.last_attempt.result()
          ~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ~~~~~~~~~~~~~~~~~^^
  File "C:\Users\Administrator\AppData\Local\Programs\Python\Python313\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\tenacity\__init__.py", line 480, in __call__
    result = fn(*args, **kwargs)
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\_api_client.py", line 1105, in _request_once
    errors.APIError.raise_for_response(response)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "D:\Learning Analytics\Learning-AI\Learning_Analytics-API\venv\Lib\site-packages\google\genai\errors.py", line 110, in raise_for_response
    raise ServerError(status_code, response_json, response)
google.genai.errors.ServerError: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
2026-01-19 15:34:09,842 - file_utils - INFO - Cleaned up file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:34:09,842 - routes - ERROR - Question generation failed: Gemini API Error: 503 UNAVAILABLE. {'error': {'code': 503, 'message': 'The model is overloaded. Please try again later.', 'status': 'UNAVAILABLE'}}
2026-01-19 15:34:09,844 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:34:09] "[32mPOST /generate HTTP/1.1[0m" 302 -
2026-01-19 15:34:09,851 - routes - INFO - Rendering Question generator page
2026-01-19 15:34:09,852 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:34:09] "GET /mcq-generator HTTP/1.1" 200 -
2026-01-19 15:36:33,151 - routes - INFO - Received Question generation request
2026-01-19 15:36:33,159 - validation - INFO - File and parameters validated successfully.
2026-01-19 15:36:33,159 - file_utils - DEBUG - File 03-storage1.pdf allowed: True
2026-01-19 15:36:33,162 - file_utils - INFO - File saved to: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:36:33,248 - pdf_extraction_util - INFO - Successfully extracted 26430 characters from 03-storage1.pdf
2026-01-19 15:36:34,058 - question_generation - INFO - QuestionGenerator initialized with model: gemini-2.5-flash
2026-01-19 15:36:34,059 - question_generation - INFO - Generating structured content using model gemini-2.5-flash...
2026-01-19 15:36:34,059 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2026-01-19 15:36:34,061 - httpcore.connection - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None
2026-01-19 15:36:34,074 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC6092250>
2026-01-19 15:36:34,074 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023EC62A9310> server_hostname='generativelanguage.googleapis.com' timeout=None
2026-01-19 15:36:34,091 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC60929C0>
2026-01-19 15:36:34,091 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2026-01-19 15:36:34,091 - httpcore.http11 - DEBUG - send_request_headers.complete
2026-01-19 15:36:34,091 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2026-01-19 15:36:34,092 - httpcore.http11 - DEBUG - send_request_body.complete
2026-01-19 15:36:34,092 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2026-01-19 15:36:57,169 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 19 Jan 2026 10:06:56 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=23066'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-19 15:36:57,170 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2026-01-19 15:36:57,170 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2026-01-19 15:36:57,171 - httpcore.http11 - DEBUG - receive_response_body.complete
2026-01-19 15:36:57,171 - httpcore.http11 - DEBUG - response_closed.started
2026-01-19 15:36:57,171 - httpcore.http11 - DEBUG - response_closed.complete
2026-01-19 15:36:57,173 - question_generation - INFO - Successfully separated 5 questions and 5 answer key items.
2026-01-19 15:36:57,174 - file_utils - INFO - Cleaned up file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:36:57,175 - pdf_generation - INFO - Creating PDF file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_03-storage1_questions.pdf
2026-01-19 15:36:57,197 - fpdf.fpdf - DEBUG - Page break on page 1 at y=275 for element of height 6 > 277
2026-01-19 15:36:57,209 - fpdf.output - DEBUG - Final size summary of the biggest document sections:
2026-01-19 15:36:57,209 - fpdf.output - DEBUG - - pages: 3.1KiB
2026-01-19 15:36:57,209 - fpdf.output - DEBUG - - fonts: 304.0B
2026-01-19 15:36:57,212 - pdf_generation - INFO - PDF successfully generated at D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_03-storage1_questions.pdf
2026-01-19 15:36:57,213 - file_utils - INFO - Saving TXT results for: 03-storage1.pdf
2026-01-19 15:36:57,214 - pdf_generation - INFO - Saving merged questions to text file: generated_mcqs_03-storage1_key.txt
2026-01-19 15:36:57,216 - pdf_generation - INFO - Successfully saved 5 questions to text file.
2026-01-19 15:36:57,217 - file_utils - INFO - TXT file saved: generated_mcqs_03-storage1_key.txt
2026-01-19 15:36:57,217 - routes - INFO - Successfully generated 5 questions.
2026-01-19 15:36:57,227 - httpcore.connection - DEBUG - close.started
2026-01-19 15:36:57,228 - httpcore.connection - DEBUG - close.complete
2026-01-19 15:36:57,232 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:36:57] "POST /generate HTTP/1.1" 200 -
2026-01-19 15:38:29,026 - routes - INFO - Rendering home page
2026-01-19 15:38:29,028 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:38:29] "GET / HTTP/1.1" 200 -
2026-01-19 15:38:32,537 - routes - INFO - Rendering Question generator page
2026-01-19 15:38:32,538 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:38:32] "GET /mcq-generator HTTP/1.1" 200 -
2026-01-19 15:38:43,487 - routes - INFO - Received Question generation request
2026-01-19 15:38:43,495 - validation - INFO - File and parameters validated successfully.
2026-01-19 15:38:43,496 - file_utils - DEBUG - File 03-storage1.pdf allowed: True
2026-01-19 15:38:43,499 - file_utils - INFO - File saved to: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:38:43,576 - pdf_extraction_util - INFO - Successfully extracted 26430 characters from 03-storage1.pdf
2026-01-19 15:38:44,407 - question_generation - INFO - QuestionGenerator initialized with model: gemini-2.5-flash
2026-01-19 15:38:44,409 - question_generation - INFO - Generating structured content using model gemini-2.5-flash...
2026-01-19 15:38:44,413 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2026-01-19 15:38:44,416 - httpcore.connection - DEBUG - close.started
2026-01-19 15:38:44,416 - httpcore.connection - DEBUG - close.complete
2026-01-19 15:38:44,416 - httpcore.connection - DEBUG - close.started
2026-01-19 15:38:44,417 - httpcore.connection - DEBUG - close.complete
2026-01-19 15:38:44,419 - httpcore.connection - DEBUG - close.started
2026-01-19 15:38:44,419 - httpcore.connection - DEBUG - close.complete
2026-01-19 15:38:44,432 - httpcore.connection - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None
2026-01-19 15:38:44,492 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC6082750>
2026-01-19 15:38:44,492 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023EC6148A50> server_hostname='generativelanguage.googleapis.com' timeout=None
2026-01-19 15:38:44,506 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC6082350>
2026-01-19 15:38:44,506 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2026-01-19 15:38:44,507 - httpcore.http11 - DEBUG - send_request_headers.complete
2026-01-19 15:38:44,507 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2026-01-19 15:38:44,507 - httpcore.http11 - DEBUG - send_request_body.complete
2026-01-19 15:38:44,507 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2026-01-19 15:39:12,892 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 19 Jan 2026 10:09:12 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=28374'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-19 15:39:12,892 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2026-01-19 15:39:12,893 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2026-01-19 15:39:12,894 - httpcore.http11 - DEBUG - receive_response_body.complete
2026-01-19 15:39:12,894 - httpcore.http11 - DEBUG - response_closed.started
2026-01-19 15:39:12,894 - httpcore.http11 - DEBUG - response_closed.complete
2026-01-19 15:39:12,895 - question_generation - INFO - Successfully separated 5 questions and 5 answer key items.
2026-01-19 15:39:12,896 - file_utils - INFO - Cleaned up file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:39:12,896 - pdf_generation - INFO - Creating PDF file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_03-storage1_questions.pdf
2026-01-19 15:39:12,905 - fpdf.fpdf - DEBUG - Page break on page 1 at y=273 for element of height 6 > 277
2026-01-19 15:39:12,914 - fpdf.fpdf - DEBUG - Page break on page 2 at y=272 for element of height 6 > 277
2026-01-19 15:39:12,922 - fpdf.output - DEBUG - Final size summary of the biggest document sections:
2026-01-19 15:39:12,922 - fpdf.output - DEBUG - - pages: 4.2KiB
2026-01-19 15:39:12,922 - fpdf.output - DEBUG - - fonts: 306.0B
2026-01-19 15:39:12,927 - pdf_generation - INFO - PDF successfully generated at D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_03-storage1_questions.pdf
2026-01-19 15:39:12,929 - file_utils - INFO - Saving TXT results for: 03-storage1.pdf
2026-01-19 15:39:12,932 - pdf_generation - INFO - Saving merged questions to text file: generated_mcqs_03-storage1_key.txt
2026-01-19 15:39:12,937 - pdf_generation - INFO - Successfully saved 5 questions to text file.
2026-01-19 15:39:12,937 - file_utils - INFO - TXT file saved: generated_mcqs_03-storage1_key.txt
2026-01-19 15:39:12,937 - routes - INFO - Successfully generated 5 questions.
2026-01-19 15:39:12,946 - httpcore.connection - DEBUG - close.started
2026-01-19 15:39:12,947 - httpcore.connection - DEBUG - close.complete
2026-01-19 15:39:12,953 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:39:12] "POST /generate HTTP/1.1" 200 -
2026-01-19 15:51:16,436 - routes - INFO - Rendering home page
2026-01-19 15:51:16,437 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:51:16] "GET / HTTP/1.1" 200 -
2026-01-19 15:51:18,810 - routes - INFO - Rendering Question generator page
2026-01-19 15:51:18,811 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:51:18] "GET /mcq-generator HTTP/1.1" 200 -
2026-01-19 15:51:27,021 - routes - INFO - Received Question generation request
2026-01-19 15:51:27,027 - validation - INFO - File and parameters validated successfully.
2026-01-19 15:51:27,028 - file_utils - DEBUG - File 03-storage1.pdf allowed: True
2026-01-19 15:51:27,030 - file_utils - INFO - File saved to: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:51:27,108 - pdf_extraction_util - INFO - Successfully extracted 26430 characters from 03-storage1.pdf
2026-01-19 15:51:27,905 - question_generation - INFO - QuestionGenerator initialized with model: gemini-2.5-flash
2026-01-19 15:51:27,905 - question_generation - INFO - Generating structured content using model gemini-2.5-flash...
2026-01-19 15:51:27,906 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2026-01-19 15:51:27,907 - httpcore.connection - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None
2026-01-19 15:51:27,932 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC5FEF890>
2026-01-19 15:51:27,932 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023EC62AA490> server_hostname='generativelanguage.googleapis.com' timeout=None
2026-01-19 15:51:27,946 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC5FEF200>
2026-01-19 15:51:27,947 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2026-01-19 15:51:27,947 - httpcore.http11 - DEBUG - send_request_headers.complete
2026-01-19 15:51:27,947 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2026-01-19 15:51:27,947 - httpcore.http11 - DEBUG - send_request_body.complete
2026-01-19 15:51:27,947 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2026-01-19 15:51:53,018 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 19 Jan 2026 10:21:52 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=25058'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-19 15:51:53,018 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2026-01-19 15:51:53,018 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2026-01-19 15:51:53,018 - httpcore.http11 - DEBUG - receive_response_body.complete
2026-01-19 15:51:53,018 - httpcore.http11 - DEBUG - response_closed.started
2026-01-19 15:51:53,018 - httpcore.http11 - DEBUG - response_closed.complete
2026-01-19 15:51:53,019 - question_generation - INFO - Successfully separated 5 questions and 5 answer key items.
2026-01-19 15:51:53,019 - file_utils - INFO - Cleaned up file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\03-storage1.pdf
2026-01-19 15:51:53,019 - pdf_generation - INFO - Creating PDF file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_03-storage1_questions.pdf
2026-01-19 15:51:53,026 - fpdf.fpdf - DEBUG - Page break on page 1 at y=275 for element of height 6 > 277
2026-01-19 15:51:53,027 - fpdf.output - DEBUG - Final size summary of the biggest document sections:
2026-01-19 15:51:53,027 - fpdf.output - DEBUG - - pages: 2.6KiB
2026-01-19 15:51:53,027 - fpdf.output - DEBUG - - fonts: 199.0B
2026-01-19 15:51:53,028 - pdf_generation - INFO - PDF successfully generated at D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_03-storage1_questions.pdf
2026-01-19 15:51:53,028 - file_utils - INFO - Saving TXT results for: 03-storage1.pdf
2026-01-19 15:51:53,028 - pdf_generation - INFO - Saving merged questions to text file: generated_mcqs_03-storage1_key.txt
2026-01-19 15:51:53,028 - pdf_generation - INFO - Successfully saved 5 questions to text file.
2026-01-19 15:51:53,028 - file_utils - INFO - TXT file saved: generated_mcqs_03-storage1_key.txt
2026-01-19 15:51:53,028 - routes - INFO - Successfully generated 5 questions.
2026-01-19 15:51:53,032 - httpcore.connection - DEBUG - close.started
2026-01-19 15:51:53,033 - httpcore.connection - DEBUG - close.complete
2026-01-19 15:51:53,036 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:51:53] "POST /generate HTTP/1.1" 200 -
2026-01-19 15:52:48,969 - routes - INFO - Rendering home page
2026-01-19 15:52:48,970 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:52:48] "GET / HTTP/1.1" 200 -
2026-01-19 15:52:50,653 - routes - INFO - Rendering Question generator page
2026-01-19 15:52:50,654 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:52:50] "GET /mcq-generator HTTP/1.1" 200 -
2026-01-19 15:52:58,566 - routes - INFO - Received Question generation request
2026-01-19 15:52:58,571 - validation - INFO - File and parameters validated successfully.
2026-01-19 15:52:58,572 - file_utils - DEBUG - File 05-storage2.pdf allowed: True
2026-01-19 15:52:58,576 - file_utils - INFO - File saved to: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\05-storage2.pdf
2026-01-19 15:52:58,704 - pdf_extraction_util - INFO - Successfully extracted 31904 characters from 05-storage2.pdf
2026-01-19 15:52:59,562 - question_generation - INFO - QuestionGenerator initialized with model: gemini-2.5-flash
2026-01-19 15:52:59,564 - question_generation - INFO - Generating structured content using model gemini-2.5-flash...
2026-01-19 15:52:59,565 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2026-01-19 15:52:59,569 - httpcore.connection - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None
2026-01-19 15:52:59,586 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC62A6890>
2026-01-19 15:52:59,586 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023EC62A85F0> server_hostname='generativelanguage.googleapis.com' timeout=None
2026-01-19 15:52:59,604 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC62A5EF0>
2026-01-19 15:52:59,604 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2026-01-19 15:52:59,604 - httpcore.http11 - DEBUG - send_request_headers.complete
2026-01-19 15:52:59,604 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2026-01-19 15:52:59,604 - httpcore.http11 - DEBUG - send_request_body.complete
2026-01-19 15:52:59,605 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2026-01-19 15:53:11,669 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 19 Jan 2026 10:23:11 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=12052'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-19 15:53:11,670 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2026-01-19 15:53:11,670 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2026-01-19 15:53:11,671 - httpcore.http11 - DEBUG - receive_response_body.complete
2026-01-19 15:53:11,672 - httpcore.http11 - DEBUG - response_closed.started
2026-01-19 15:53:11,672 - httpcore.http11 - DEBUG - response_closed.complete
2026-01-19 15:53:11,673 - question_generation - INFO - Successfully separated 5 questions and 5 answer key items.
2026-01-19 15:53:11,675 - file_utils - INFO - Cleaned up file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\05-storage2.pdf
2026-01-19 15:53:11,675 - pdf_generation - INFO - Creating PDF file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_05-storage2_questions.pdf
2026-01-19 15:53:11,684 - fpdf.output - DEBUG - Final size summary of the biggest document sections:
2026-01-19 15:53:11,684 - fpdf.output - DEBUG - - pages: 927.0B
2026-01-19 15:53:11,684 - fpdf.output - DEBUG - - fonts: 199.0B
2026-01-19 15:53:11,685 - pdf_generation - INFO - PDF successfully generated at D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_05-storage2_questions.pdf
2026-01-19 15:53:11,685 - file_utils - INFO - Saving TXT results for: 05-storage2.pdf
2026-01-19 15:53:11,685 - pdf_generation - INFO - Saving merged questions to text file: generated_mcqs_05-storage2_key.txt
2026-01-19 15:53:11,686 - pdf_generation - INFO - Successfully saved 5 questions to text file.
2026-01-19 15:53:11,686 - file_utils - INFO - TXT file saved: generated_mcqs_05-storage2_key.txt
2026-01-19 15:53:11,686 - routes - INFO - Successfully generated 5 questions.
2026-01-19 15:53:11,690 - httpcore.connection - DEBUG - close.started
2026-01-19 15:53:11,690 - httpcore.connection - DEBUG - close.complete
2026-01-19 15:53:11,694 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:53:11] "POST /generate HTTP/1.1" 200 -
2026-01-19 15:54:47,097 - routes - INFO - Rendering home page
2026-01-19 15:54:47,098 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:54:47] "GET / HTTP/1.1" 200 -
2026-01-19 15:54:48,954 - routes - INFO - Rendering Question generator page
2026-01-19 15:54:48,955 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:54:48] "GET /mcq-generator HTTP/1.1" 200 -
2026-01-19 15:54:56,567 - routes - INFO - Received Question generation request
2026-01-19 15:54:56,576 - validation - INFO - File and parameters validated successfully.
2026-01-19 15:54:56,576 - file_utils - DEBUG - File 05-storage2.pdf allowed: True
2026-01-19 15:54:56,579 - file_utils - INFO - File saved to: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\05-storage2.pdf
2026-01-19 15:54:56,728 - pdf_extraction_util - INFO - Successfully extracted 31904 characters from 05-storage2.pdf
2026-01-19 15:54:57,511 - question_generation - INFO - QuestionGenerator initialized with model: gemini-2.5-flash
2026-01-19 15:54:57,511 - question_generation - INFO - Generating structured content using model gemini-2.5-flash...
2026-01-19 15:54:57,511 - google_genai.models - INFO - AFC is enabled with max remote calls: 10.
2026-01-19 15:54:57,513 - httpcore.connection - DEBUG - connect_tcp.started host='generativelanguage.googleapis.com' port=443 local_address=None timeout=None socket_options=None
2026-01-19 15:54:57,529 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC6099710>
2026-01-19 15:54:57,529 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000023EC6149310> server_hostname='generativelanguage.googleapis.com' timeout=None
2026-01-19 15:54:57,543 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000023EC5F1AF90>
2026-01-19 15:54:57,543 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2026-01-19 15:54:57,543 - httpcore.http11 - DEBUG - send_request_headers.complete
2026-01-19 15:54:57,543 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2026-01-19 15:54:57,543 - httpcore.http11 - DEBUG - send_request_body.complete
2026-01-19 15:54:57,543 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2026-01-19 15:55:08,569 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'application/json; charset=UTF-8'), (b'Vary', b'Origin'), (b'Vary', b'X-Origin'), (b'Vary', b'Referer'), (b'Content-Encoding', b'gzip'), (b'Date', b'Mon, 19 Jan 2026 10:25:08 GMT'), (b'Server', b'scaffolding on HTTPServer2'), (b'X-XSS-Protection', b'0'), (b'X-Frame-Options', b'SAMEORIGIN'), (b'X-Content-Type-Options', b'nosniff'), (b'Server-Timing', b'gfet4t7; dur=11014'), (b'Alt-Svc', b'h3=":443"; ma=2592000,h3-29=":443"; ma=2592000'), (b'Transfer-Encoding', b'chunked')])
2026-01-19 15:55:08,569 - httpx - INFO - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent "HTTP/1.1 200 OK"
2026-01-19 15:55:08,570 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2026-01-19 15:55:08,570 - httpcore.http11 - DEBUG - receive_response_body.complete
2026-01-19 15:55:08,570 - httpcore.http11 - DEBUG - response_closed.started
2026-01-19 15:55:08,571 - httpcore.http11 - DEBUG - response_closed.complete
2026-01-19 15:55:08,572 - question_generation - INFO - Successfully separated 5 questions and 5 answer key items.
2026-01-19 15:55:08,573 - file_utils - INFO - Cleaned up file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\uploads\05-storage2.pdf
2026-01-19 15:55:08,573 - pdf_generation - INFO - Creating PDF file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_05-storage2_questions.pdf
2026-01-19 15:55:08,588 - fpdf.output - DEBUG - Final size summary of the biggest document sections:
2026-01-19 15:55:08,588 - fpdf.output - DEBUG - - pages: 1.4KiB
2026-01-19 15:55:08,588 - fpdf.output - DEBUG - - fonts: 199.0B
2026-01-19 15:55:08,589 - pdf_generation - INFO - PDF successfully generated at D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_05-storage2_questions.pdf
2026-01-19 15:55:08,589 - file_utils - INFO - Saving TXT results for: 05-storage2.pdf
2026-01-19 15:55:08,589 - pdf_generation - INFO - Saving merged questions to text file: generated_mcqs_05-storage2_key.txt
2026-01-19 15:55:08,590 - pdf_generation - INFO - Successfully saved 5 questions to text file.
2026-01-19 15:55:08,590 - file_utils - INFO - TXT file saved: generated_mcqs_05-storage2_key.txt
2026-01-19 15:55:08,590 - routes - INFO - Successfully generated 5 questions.
2026-01-19 15:55:08,599 - httpcore.connection - DEBUG - close.started
2026-01-19 15:55:08,599 - httpcore.connection - DEBUG - close.complete
2026-01-19 15:55:08,602 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:55:08] "POST /generate HTTP/1.1" 200 -
2026-01-19 15:56:41,756 - routes - INFO - Download request for: generated_mcqs_05-storage2_questions.pdf
2026-01-19 15:56:41,757 - routes - INFO - Sending file: D:\Learning Analytics\Learning-AI\Question_and_Answer_Generation_with_Feeddack\results\generated_mcqs_05-storage2_questions.pdf
2026-01-19 15:56:41,758 - werkzeug - INFO - 127.0.0.1 - - [19/Jan/2026 15:56:41] "GET /download/generated_mcqs_05-storage2_questions.pdf HTTP/1.1" 200 -
2026-01-19 16:08:52,940 - werkzeug - INFO -  * Detected change in 'D:\\Learning Analytics\\Learning-AI\\Question_and_Answer_Generation_with_Feeddack\\app\\database.py', reloading
2026-01-19 16:08:52,942 - ragas._analytics - DEBUG - AnalyticsBatcher shutdown complete
